{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3_Ossan_Noémie_Yosr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture du fichier csv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Création d'un dataFrame tweet et sentiment, on ignore les 10 premières lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('task3-train.csv', sep='\\t', header=None, skiprows=10)\n",
    "df1 = df.iloc[:,-2:] # get two lasts columns\n",
    "df1.columns = [\"text\",\"sentiment\"] # name columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation du corpus (70/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Séparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, sentiment_train, sentiment_test = train_test_split(df1.text, df1.sentiment, test_size=0.3,random_state=109) # 70% training and 30% test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Utilisation de BagOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(data_train)\n",
    "test_vectors = vectorizer.transform(data_test) # check difference\n",
    "##il faut obtenir l'id du test, et la prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du modèle à partir des données d'apprentissages"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Choix du modèle SVM (linear )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6196721311475409\n",
      "Macro average:  {'precision': 0.5411369037245206, 'recall': 0.5223835187433795, 'f1-score': 0.5303524234436784, 'support': 1525}\n",
      "<==\n",
      " FOR EACH CLASS ==>\n",
      "\n",
      "positive:  {'precision': 0.5700934579439252, 'recall': 0.4728682170542636, 'f1-score': 0.516949152542373, 'support': 129}\n",
      "negative:  {'precision': 0.6597222222222222, 'recall': 0.6785714285714286, 'f1-score': 0.6690140845070424, 'support': 700}\n",
      "mixed:  {'precision': 0.24242424242424243, 'recall': 0.21739130434782608, 'f1-score': 0.22922636103151864, 'support': 184}\n",
      "objective:  {'precision': 0.6923076923076923, 'recall': 0.720703125, 'f1-score': 0.7062200956937799, 'support': 512}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "\n",
    "#Train Data vectors\n",
    "classifier_linear.fit(train_vectors, sentiment_train)\n",
    "\n",
    "\n",
    "#Prediction\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "\n",
    "report = classification_report(sentiment_test, prediction_linear, output_dict=True)\n",
    "print('Accuracy: ', report['accuracy'])\n",
    "print('Macro average: ', report['macro avg'])\n",
    "\n",
    "print('<==\\n FOR EACH CLASS ==>\\n')\n",
    "\n",
    "print('positive: ', report['positive'])\n",
    "print('negative: ', report['negative'])\n",
    "print('mixed: ', report['mixed'])\n",
    "print('objective: ', report['objective'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du modèle à partir du corpus complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9994095650462508\n",
      "Macro average:  {'precision': 0.9993084835979785, 'recall': 0.9990582876752612, 'f1-score': 0.9991832062726396, 'support': 5081}\n",
      "\n",
      " <==FOR EACH CLASS ==>\n",
      "\n",
      "positive:  {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 498}\n",
      "negative:  {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2251}\n",
      "mixed:  {'precision': 0.9984076433121019, 'recall': 0.9968203497615262, 'f1-score': 0.9976133651551312, 'support': 629}\n",
      "objective:  {'precision': 0.9988262910798122, 'recall': 0.9994128009395185, 'f1-score': 0.9991194599354271, 'support': 1703}\n"
     ]
    }
   ],
   "source": [
    "#Train on whole dataset\n",
    "train_vectors = vectorizer.fit_transform(df1.text)\n",
    "test_vector = vectorizer.fit_transform(df1.text)\n",
    "classifier_linear.fit(train_vectors, df1.sentiment)\n",
    "\n",
    "\n",
    "#Prediction\n",
    "prediction_linear = classifier_linear.predict(test_vector)\n",
    "\n",
    "report = classification_report(df1.sentiment, prediction_linear, output_dict=True)\n",
    "\n",
    "print('Accuracy: ', report['accuracy'])\n",
    "print('Macro average: ', report['macro avg'])\n",
    "\n",
    "print('\\n <==FOR EACH CLASS ==>\\n')\n",
    "\n",
    "print('positive: ', report['positive'])\n",
    "print('negative: ', report['negative'])\n",
    "print('mixed: ', report['mixed'])\n",
    "print('objective: ', report['objective'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run2 with TFIDF / SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6577049180327869\n",
      "Macro average:  {'precision': 0.5732139470271697, 'recall': 0.49586950409113345, 'f1-score': 0.5033350769499593, 'support': 1525}\n",
      "<==\n",
      " FOR EACH CLASS ==>\n",
      "\n",
      "positive:  {'precision': 0.6363636363636364, 'recall': 0.3798449612403101, 'f1-score': 0.4757281553398058, 'support': 129}\n",
      "negative:  {'precision': 0.64472190692395, 'recall': 0.8114285714285714, 'f1-score': 0.7185325743200506, 'support': 700}\n",
      "mixed:  {'precision': 0.3055555555555556, 'recall': 0.059782608695652176, 'f1-score': 0.1, 'support': 184}\n",
      "objective:  {'precision': 0.7062146892655368, 'recall': 0.732421875, 'f1-score': 0.7190795781399808, 'support': 512}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df = 5,max_df = 0.8,sublinear_tf = True,use_idf = True)\n",
    "train_vectors = vectorizer.fit_transform(data_train)\n",
    "test_vectors = vectorizer.transform(data_test) # check difference\n",
    "##il faut obtenir l'id du test, et la prediction\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "\n",
    "#Train Data vectors\n",
    "classifier_linear.fit(train_vectors, sentiment_train)\n",
    "\n",
    "\n",
    "#Prediction\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "\n",
    "report = classification_report(sentiment_test, prediction_linear, output_dict=True)\n",
    "print('Accuracy: ', report['accuracy'])\n",
    "print('Macro average: ', report['macro avg'])\n",
    "\n",
    "print('<==\\n FOR EACH CLASS ==>\\n')\n",
    "\n",
    "print('positive: ', report['positive'])\n",
    "print('negative: ', report['negative'])\n",
    "print('mixed: ', report['mixed'])\n",
    "print('objective: ', report['objective'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run3 Stop words (French dictionary) / with TFIDF / SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
      "Accuracy:  0.6432786885245901\n",
      "Macro average:  {'precision': 0.5470516211813651, 'recall': 0.47876792033800375, 'f1-score': 0.4835763615557499, 'support': 1525}\n",
      "<==\n",
      " FOR EACH CLASS ==>\n",
      "\n",
      "positive:  {'precision': 0.6, 'recall': 0.3488372093023256, 'f1-score': 0.4411764705882353, 'support': 129}\n",
      "negative:  {'precision': 0.6330690826727067, 'recall': 0.7985714285714286, 'f1-score': 0.7062539481996211, 'support': 700}\n",
      "mixed:  {'precision': 0.2647058823529412, 'recall': 0.04891304347826087, 'f1-score': 0.08256880733944955, 'support': 184}\n",
      "objective:  {'precision': 0.6904315196998124, 'recall': 0.71875, 'f1-score': 0.7043062200956938, 'support': 512}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words_list = stopwords.words('french')\n",
    "print(stop_words_list)\n",
    "vectorizer = TfidfVectorizer(min_df = 5,max_df = 0.8,sublinear_tf = True,use_idf = True, stop_words=stop_words_list)\n",
    "train_vectors = vectorizer.fit_transform(data_train)\n",
    "test_vectors = vectorizer.transform(data_test) # check difference\n",
    "##il faut obtenir l'id du test, et la prediction\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "\n",
    "#Train Data vectors\n",
    "classifier_linear.fit(train_vectors, sentiment_train)\n",
    "\n",
    "\n",
    "#Prediction\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "\n",
    "report = classification_report(sentiment_test, prediction_linear, output_dict=True)\n",
    "print('Accuracy: ', report['accuracy'])\n",
    "print('Macro average: ', report['macro avg'])\n",
    "\n",
    "print('<==\\n FOR EACH CLASS ==>\\n')\n",
    "\n",
    "print('positive: ', report['positive'])\n",
    "print('negative: ', report['negative'])\n",
    "print('mixed: ', report['mixed'])\n",
    "print('objective: ', report['objective'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Run3 moins performant que Run2, trop de stopwords supprimé ce qui explique la mauvaise performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests avec Tweet non annotés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creation a predictioncsv file\n",
    "def predict(inputFile,outputFile, model):\n",
    "    ## Get DataFrame from csv file (column => id tweet)\n",
    "    dfTest = pd.read_csv(inputFile, sep='\\t', header=None, skiprows=9) ## Check rows to Skip\n",
    "    dfTestTweet = dfTest.iloc[:,-1:] # get the last column => tweet\n",
    "    dfTestTweet = dfTestTweet.iloc[:,0] # Turn into panda Series\n",
    "    \n",
    "    #Vecteur des ids numpy array str\n",
    "    dfTestId = ((dfTest.iloc[:,0]).to_numpy()).astype(str)\n",
    "    #Création du vecteur de données tests\n",
    "    test_vectors = vectorizer.transform(dfTestTweet) \n",
    "    \n",
    "    #Prediction\n",
    "    prediction = model.predict(test_vectors)\n",
    "    \n",
    "    #Create file lines \n",
    "    for i in range(dfTestId.size):\n",
    "        dfTestId[i]= \"(\" + dfTestId[i] + \")\"\n",
    "        \n",
    "    matrix = np.column_stack((prediction,dfTestId))\n",
    "    \n",
    "    #Create the file\n",
    "    np.savetxt(outputFile, matrix ,fmt='%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP => Changer le nom du fichier csv et le classifier\n",
    "predict('task1-test_extrait20.csv','resultat1.sc',classifier_linear)\n",
    "predict('task2-test_extrait20.csv','resultat2.sc',classifier_linear)\n",
    "predict('task3-test_extrait20.csv','resultat3.sc',classifier_linear)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
